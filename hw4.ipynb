{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junzeye/stats305b_LLM/blob/main/hw4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISL7lGbExVbF"
      },
      "source": [
        "# HW4: Large Language Models\n",
        "\n",
        "The first half of this assignment (Parts 0 and 1) will review some key ingredients of sequence modeling. In the process, we will build a baseline transformer model for next token prediction in code.\n",
        "**The deliverable will be completing the questions posed in part 0 and part 1.**\n",
        "\n",
        "The second half of the assignment (Part 2) will be an open-ended mini-project where you have the freedom to delve more deeply into language modelling (where the language in question is python code). Further instructions are in _Part 2: Mini-project_. But, in general, you should feel free to try other architectures (HMMs, RNNs, transformers, state space layers, diffusion models etc.) or to invent new architectures. The goal will be to find some area of possible improvement (we interpret \"improvement\" quite loosely, but it is up to you to state precisely in what sense your proposed innovation might constitute an improvement and to show convincing evidence that your innovation does or does not constitue an improvement according to your definition); to formulate and state a precise hypothesis; and to falsify or support the hypothesis with rigorous empirical analyses.\n",
        "**The deliverable will be a report of no more than 4 pages (references not included in the page limit).**\n",
        "\n",
        "**For this final assignment you have the option to work in pairs.**\n",
        "\n",
        "\n",
        "> **<u>This Assignment</u>**\n",
        ">\n",
        ">**Model:** You will begin by implementing a baseline using attention and transformers. But, for the mini-project, you will be free to use any model (HMMs, RNNs, transformers, state space layers, diffusion models etc.) that you would like!\n",
        ">\n",
        ">**Algorithm:** mini-batched stochatic gradient descent / whatever you'd like! We will be using deep learning models in at least the first half of the assignment, so be sure to **use the GPU on colab** (make sure you switch to GPU in the \"Runtime\" tab above).\n",
        ">\n",
        ">**Data**: A large corpus of python code from [the Stack](https://huggingface.co/datasets/bigcode/the-stack-dedup). We have taken a dataset of around 4 million tokens from the stack and stored in a csv file for you for easy access."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ-AwbZ4ySCJ"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Xd4fRbWr15Yp"
      },
      "outputs": [],
      "source": [
        "# can take around 30s\n",
        "%%capture\n",
        "! pip install datasets #huggingface datasets library\n",
        "! pip install --upgrade pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_5IXGh6OOBZV"
      },
      "outputs": [],
      "source": [
        "# torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# hugging face imports\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import sys\n",
        "import warnings\n",
        "\n",
        "torch.manual_seed(305)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# assert device=='cuda', \"you need to change runtime type to GPU\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "A_Z5Jh74DH_E"
      },
      "outputs": [],
      "source": [
        "# hyperparams and helper functions\n",
        "SMALL_ITERS = 1000\n",
        "LARGE_ITERS = 2000\n",
        "context_window_size = 256\n",
        "chunk_size = 512 # BERT can only take max input size 512 characters\n",
        "\n",
        "def chunk_string(string, size):\n",
        "    \"\"\"\n",
        "    Splits a string into chunks of a specified size.\n",
        "\n",
        "    :param string: The string to be chunked.\n",
        "    :param size: The desired chunk size.\n",
        "    :return: A list of string chunks.\n",
        "    \"\"\"\n",
        "    return [string[i:i+size] for i in range(0, len(string), size)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF6dgHnhOprg"
      },
      "source": [
        "## Part 0: Preprocessing\n",
        "\n",
        "As in the previous problem sets, a certain amount of preprocessing for textual data is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF0_bhXNOxeS"
      },
      "source": [
        "### 0.1: Loading the dataset\n",
        "\n",
        "The first step is to actually download the dataset. We will be using a dataset on [huggingface](https://huggingface.co/). You can think of hugging face as the sklearn of deep learning.\n",
        "\n",
        "The dominant mode for preprocessing textual data is to tokenize it, that is, to split the dataset into a finite vocabulary of tokens. Then, we can set up a dictionary where counting numbers map to tokens. Tokens can be characters, or words, or subwords; in fact, the \"best\" way to tokenize text is an active area of research. For our baseline, we will use a tokenizer that microsoft created for code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "53dGz7ExDkUv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304,
          "referenced_widgets": [
            "0078e04487ec4095a3d5bbb7f24db3a6",
            "628f511e1d904726a3e9638fef23d665",
            "4a0442732c4f4d9fb43a0903b5a8370d",
            "1a5d9bd787a94b548a2be19a9dfed185",
            "64301a962c25422ba3c1fe2659ae5415",
            "efa2cef7e558446aa0503d20b2bf8fc4",
            "9151ddbea4ee48499b79c0ab1a7c17d2",
            "777457e679114d1587314d165e68302f",
            "a2cabd697aa145c693be3c194c3bda9e",
            "be6dfc1abe5b4b9e801e458840524924",
            "ce86e53f9a664bcfb6a4449de6c8c9b7",
            "f66a194580fa474a8d95354a402dbed3",
            "b59a009124604a54bf87dcaeb39fbec1",
            "e0d12605e10a40598a65adc10df89db9",
            "09a9125590604af3880fb3f543c22bdb",
            "e39f82097a914a7c9318a44db253252a",
            "def03c6018f14663b69fcba4b453474b",
            "978832e8e5594382a205286f2f6717dd",
            "493ac37e4d604c19ac33e8fdb08fbc94",
            "bc4605cbeeda4ae59fff1b355f10d392",
            "2aebfe85937d447b81827c79475c98ce",
            "814c99ede2054ad8bafb2d5c97480d48",
            "eecc20acd520448a9a81f1a63dcfa774",
            "77c5aa3b095f48d3ba7e733046c39ec0",
            "c3d5ac522ec3408a914d79beb748fe01",
            "8e204b2c650e497d97f573ce0764ed2d",
            "da1f8541033446bb88763d6195ac2c1d",
            "26f6acc6fd1f4d8d9c297b35b6747f8d",
            "8fb7851650964f0fa371c96f42fc8e7b",
            "a4e0f1948aa14a4c826e4ac8d2a97f52",
            "23e6a31b25cf4193b8267d971e0e9c4b",
            "5e35d3fbb880488a95e08bb76629e4c8",
            "7990f0259e234e62be5dc4b25f823870",
            "cdf2640810bf4ad2be623e635cee8f36",
            "cb57cd7623814ebe8eaae987903cedfc",
            "85017df8d6bb4ebebd8f3ee4d3d94796",
            "5656e341dd3c4059b1ceeea1766f0c02",
            "5bdebb83b6c04b8ca50aa3a53dde58ac",
            "e4b0ccb525614f0295e145b3a98da65a",
            "beedbc83ae594b5b9c6d51528c682ac5",
            "fc545a240a0c43e48b9bf8b55e2b2f28",
            "67a393166d4b4e1f865e1be69f7c1bb0",
            "53961f6d0fa947d1b8ccc2571d5025ad",
            "9a56c0878d25429aba20b5f6dfdd7392",
            "b1dbc14a7ae64dd588252ed595a09c20",
            "1ebe0e395a3843c6873c129ba1cee2de",
            "38cd9b74f0fb4dba9608ac5c4c2db42a",
            "08848c184d1544f886aac6aad95b064a",
            "836aa82ea12143e6ac118ddb55456ca2",
            "8efcb07b65334270a203e1376713af54",
            "532f0155ecbe46589da10aae684754ae",
            "ce111c7acd7b4bd0b360dd76c0c58b61",
            "75acf48645194d4d9e9cbdb01132ed64",
            "25c5de389304432c9d44c266ec686297",
            "ae16294f1a2f404f8c4fa9626739c3c9"
          ]
        },
        "outputId": "8494bc9d-4fcc-4654-8c13-c64bc5e6a899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0078e04487ec4095a3d5bbb7f24db3a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f66a194580fa474a8d95354a402dbed3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eecc20acd520448a9a81f1a63dcfa774"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdf2640810bf4ad2be623e635cee8f36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1dbc14a7ae64dd588252ed595a09c20"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('microsoft/CodeBERT-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BAdrvcIbORig"
      },
      "outputs": [],
      "source": [
        "# Load the concatenated data\n",
        "raw_data = pd.read_csv(\"https://raw.githubusercontent.com/slinderman/stats305b/winter2024/assignments/hw4/python_corpus_4M.csv\", header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LFZ2or4BO2w3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22655a9d-6f74-435c-8864-37ed055bd968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4000596 tokens have been loaded in\n"
          ]
        }
      ],
      "source": [
        "# should take around 3 min to load in around 4M tokens\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "tokens = torch.tensor([], dtype=torch.long)\n",
        "for index, row in raw_data.iterrows():\n",
        "    text = row[0]\n",
        "    chunks = chunk_string(text, chunk_size)\n",
        "    n = len(chunks)\n",
        "    for idx, chunk in enumerate(chunks):\n",
        "        new_tokens = torch.tensor(tokenizer.encode(chunk, add_special_tokens=True))\n",
        "\n",
        "        # logic to avoid incorrectly adding in start and end sequence tokens as an artifact of chunking\n",
        "        if idx == 0:\n",
        "            tokens = torch.cat((tokens, new_tokens[:-1]), dim=0)\n",
        "        elif idx == n-1:\n",
        "            tokens = torch.cat((tokens, new_tokens[1:]), dim=0)\n",
        "        else:\n",
        "            tokens = torch.cat((tokens, new_tokens[1:-1]), dim=0)\n",
        "\n",
        "print(f\"{len(tokens)} tokens have been loaded in\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUnScCoUCRI0"
      },
      "source": [
        "### Question 0.2: Examining the tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rz0XmuXCdGJ"
      },
      "source": [
        "Let's see what the tokens look like! We will use these two prompts during the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KPbmLgmmChVg"
      },
      "outputs": [],
      "source": [
        "prompt_1_text = \\\n",
        "\"\"\"def newton(eta, N, X, y, gamma, beta=None):\n",
        "  \\\"\"\"\n",
        "  Performs Newton's method on the negative average log likelihood with an\n",
        "  l2 regularization term\n",
        "\n",
        "  beta: torch.Tensor, of shape (teams)\n",
        "  X: torch.Tensor, the covariate matrix, of shape (-1, teams)\n",
        "  y: torch.Tensor, the response vector, of shape (teams)\n",
        "  gamma: float, the scale parameter for the regularization\n",
        "  beta: torch.Tensor, the starting point for gradient descent, if specified\n",
        "  \\\"\"\"\n",
        "\n",
        "  if beta is None:\n",
        "    # Instantiate the beta vector at a random point\n",
        "    beta = torch.randn(X.shape[1])\n",
        "  else:\n",
        "    beta = torch.clone(beta)\n",
        "\n",
        "  loss = []\n",
        "\n",
        "  # Instantiate a list to store the loss throughout the gradient descent\n",
        "  # path\n",
        "  for i in tqdm(range(N)):\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gZpeZjvqCnmH"
      },
      "outputs": [],
      "source": [
        "prompt_2_text = \\\n",
        "\"\"\"import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def normalize(x, axis=-1):\n",
        "    \\\"\"\"Performs L2-Norm.\\\"\"\"\n",
        "    num = x\n",
        "    denom = torch.norm(x, 2, axis, keepdim=True).expand_as(x) + 1e-12\n",
        "    return num / denom\n",
        "\n",
        "def euclidean_dist(x, y):\n",
        "    \\\"\"\"Computes Euclidean distance.\\\"\"\"\n",
        "    m, n = x.size(0), y.size(0)\n",
        "    xx = torch.pow(x, 2).sum(1, keepdim=True).expand(m, n)\n",
        "    yy = torch.pow(x, 2).sum(1, keepdim=True).expand(m, m).t()\n",
        "    dist = xx + yy - 2 * torch.matmul(x, y.t())\n",
        "\n",
        "    dist = dist.clamp(min=1e-12).sqrt()\n",
        "\n",
        "    return dist\n",
        "\n",
        "\n",
        "def cosine_dist(x, y):\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFZUT77rCp3q"
      },
      "source": [
        "Here is what the tokenized output for the prompts looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-8rJjbTKCtRa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "b4e18aa7-630d-4c70-8c02-66b88818d8d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s>def newton(eta, N, X, y, gamma, beta=None):\\n  \"\"\"\\n  Performs Newton\\'s method on the negative average log likelihood with an\\n  l2 regularization term\\n\\n  beta: torch.Tensor, of shape (teams)\\n  X: torch.Tensor, the covariate matrix, of shape (-1, teams)\\n  y: torch.Tensor, the response vector, of shape (teams)\\n  gamma: float, the scale parameter for the regularization\\n  beta: torch.Tensor, the starting point for gradient descent, if specified\\n  \"\"\"\\n\\n  if beta is None:\\n    # Instantiate the beta vector at a random point\\n    beta = torch.randn(X.shape[1])\\n  else:\\n    beta = torch.clone(beta)\\n\\n  loss = []\\n\\n  # Instantiate a list to store the loss throughout the gradient descent\\n  # path\\n  for i in tqdm(range(N)):</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "tokenizer.decode(tokenizer.encode(prompt_1_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "a4zd13L0CvQ9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "e3ec5407-66ee-4db7-e997-c7b918550e19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s>import torch\\nimport torch.nn.functional as F\\n\\n\\ndef normalize(x, axis=-1):\\n    \"\"\"Performs L2-Norm.\"\"\"\\n    num = x\\n    denom = torch.norm(x, 2, axis, keepdim=True).expand_as(x) + 1e-12\\n    return num / denom\\n\\ndef euclidean_dist(x, y):\\n    \"\"\"Computes Euclidean distance.\"\"\"\\n    m, n = x.size(0), y.size(0)\\n    xx = torch.pow(x, 2).sum(1, keepdim=True).expand(m, n)\\n    yy = torch.pow(x, 2).sum(1, keepdim=True).expand(m, m).t()\\n    dist = xx + yy - 2 * torch.matmul(x, y.t())\\n\\n    dist = dist.clamp(min=1e-12).sqrt()\\n\\n    return dist\\n\\n\\ndef cosine_dist(x, y):</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "tokenizer.decode(tokenizer.encode(prompt_2_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbOwt9U_CyNi"
      },
      "source": [
        "And here are what the first and last 10 tokens for prompt 1 look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nqBUhCiUC6Tk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40661773-5d43-4406-c941-1524bc529215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 : <s>\n",
            "9232 : def\n",
            "92 :  new\n",
            "1054 : ton\n",
            "1640 : (\n",
            "8152 : eta\n",
            "6 : ,\n",
            "234 :  N\n",
            "6 : ,\n",
            "1577 :  X\n"
          ]
        }
      ],
      "source": [
        "for tok in tokenizer.encode(prompt_1_text, add_special_tokens=True)[:10]:\n",
        "    print(f\"{tok} : {tokenizer.decode([tok])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UHpBvaxzC7hd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "028f7f88-ebd1-4468-ad37-4cd2b1358be9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "326 :  t\n",
            "1343 : q\n",
            "43604 : dm\n",
            "1640 : (\n",
            "9435 : range\n",
            "1640 : (\n",
            "487 : N\n",
            "43 : )\n",
            "3256 : ):\n",
            "2 : </s>\n"
          ]
        }
      ],
      "source": [
        "for tok in tokenizer.encode(prompt_1_text)[-10:]:\n",
        "    print(f\"{tok} : {tokenizer.decode([tok])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrBJPZYYC9za"
      },
      "source": [
        "**Question 0.2**: What is the meanining of the `<s>` and the `<\\s>` tokens? Why is it useful to have them?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** The \\<s> and \\</s> tokens are special markers used in the context of language model training and processing to indicate the beginning and end of a sentence or text sequence, respectively. This is crucial for models to understand where a sentence or a coherent piece of text begins and ends, especially when processing multiple sentences or paragraphs in a single input. In addition, these tokens can signal different stages of processing or be used to trigger specific behaviors. For instance, in sequence-to-sequence models, the \\<s> token might signal the start of a generation task, while \\</s> indicates that the model should stop generating further text."
      ],
      "metadata": {
        "id": "VFUjm1Em6cON"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hwg1_7kDxwY"
      },
      "source": [
        "### 0.3: Building our dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk4UJm5SEMlB"
      },
      "source": [
        "There are around 50,000 tokens in the codebert vocab, but we only use around 20,000 of them. To make our lives easier, we just reindex the token indices to go from 1 to around 20,000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "RZC0TYeaD6yj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c621db0f-667e-493d-88d6-740cf99e71a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there are 21970 distinct tokens in the vocabulary\n"
          ]
        }
      ],
      "source": [
        "# Get unique elements\n",
        "extra_tokens = torch.cat((torch.tensor(tokenizer.encode(prompt_1_text, add_special_tokens=True)),\n",
        "                          torch.tensor(tokenizer.encode(prompt_2_text, add_special_tokens=True))),\n",
        "                         dim=0)\n",
        "\n",
        "unique_tokens = torch.unique(torch.cat((tokens, extra_tokens), dim=0))\n",
        "\n",
        "# Create a mapping from code bert to ids that increment by one\n",
        "from_code_bert_dict = {element.item(): id for id, element in enumerate(unique_tokens)}\n",
        "\n",
        "# Create a reverse mapping from ids to code bert token ids\n",
        "to_code_bert_dict = {id: element for element, id in from_code_bert_dict.items()}\n",
        "\n",
        "vocab_size = len(unique_tokens)\n",
        "print(f\"there are {vocab_size} distinct tokens in the vocabulary\")\n",
        "\n",
        "# helper functions to move between code bert and simple ids\n",
        "def from_code_bert(tkn_lst):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    tkn_lst: a list of code bert tokens\n",
        "    Returns:\n",
        "    a list of simple ids\n",
        "    \"\"\"\n",
        "    tkns = [int(from_code_bert_dict[token]) for token in tkn_lst]\n",
        "    return tkns\n",
        "\n",
        "\n",
        "def to_code_bert(tkn_lst):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    tkn_lst: a list of simple ids\n",
        "    Returns:\n",
        "    a list of code bert tokens\n",
        "    \"\"\"\n",
        "    tkns = [int(to_code_bert_dict[token]) for token in tkn_lst]\n",
        "    return tkns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pNOmdccJEXWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baa6d7b3-8ec0-4790-8bf8-43f050557181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there are 3600536 tokens in the training set\n",
            "there are 400060 tokens in the validation set\n",
            "there are 21970 distinct tokens in the vocabulary\n"
          ]
        }
      ],
      "source": [
        "# let's translate our dataset into our ids\n",
        "tokens_simple_id = torch.tensor([from_code_bert_dict[token.item()] for token in tokens])\n",
        "\n",
        "# split up the data into train and validation sets\n",
        "n = int(0.9 * len(tokens_simple_id)) # first 90% will be train, rest val\n",
        "train_data = tokens_simple_id.clone()[:n]\n",
        "val_data = tokens_simple_id.clone()[n:]\n",
        "\n",
        "print(f\"there are {len(train_data)} tokens in the training set\")\n",
        "print(f\"there are {len(val_data)} tokens in the validation set\")\n",
        "print(f\"there are {vocab_size} distinct tokens in the vocabulary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsOs0_diEcjY"
      },
      "source": [
        "We also write helper functions to get batches of data and to evaluate the loss of various models on them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AkAD0PfiEfjG"
      },
      "outputs": [],
      "source": [
        "# function for getting batches of data\n",
        "def get_batch(split, context_window_size, device, batch_size=32):\n",
        "    \"\"\"\n",
        "    generate a small batch of data of inputs x and targets y\n",
        "\n",
        "    Args:\n",
        "        split: 'train' or 'val'\n",
        "        device: 'cpu' or 'cuda' (should be 'cuda' if available)\n",
        "    \"\"\"\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - context_window_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+context_window_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+context_window_size+1] for i in ix])\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# helper function for tracking loss during training\n",
        "# given to you\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, eval_iters, context_window_size, device):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      model: model being evaluated\n",
        "      eval_iters: number of batches to average over\n",
        "      context_window_size: size of the context window\n",
        "      device: 'cpu' or 'cuda' (should be 'cuda' if available)\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split, context_window_size, device)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ercCUt_d07FX"
      },
      "source": [
        "## Part 1: Language Modeling\n",
        "\n",
        "In this first part of the assignment, we will implement a baseline for code modeling.\n",
        "\n",
        "In the process of building this baseline, we will review 4 key ideas of sequence modeling that have become the backbone of modern language modeling such as ChatGPT:\n",
        "\n",
        "1. Framing language modeling as next token prediction, and next token prediction as multiclass logistic regression\n",
        "2. Embedding discrete tokens in continuous latent spaces (word embeddings)\n",
        "3. Use the attention mechanism to move beyond Markovian models for sequences (we of course pay for this greater expressivity with increased compute, which is made possible in part by using matrix multiplications on acccelerated hardware like GPUs. Reducing the compute burden while maintaining the expressivity needed for good sequence modeling is an active area of research).\n",
        "4. Combining attention with deep learning in the Transformer architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh1kX0pw1bR2"
      },
      "source": [
        "### 1.1: Next token prediction as multiclass logistic regression\n",
        "\n",
        "Our first language model will simply be a lookup table. That is, given that we have token with value $v$, we will simply \"look up\" the logits that correspond to our prediction for the next token. This model is often known as a \"bigram model\" because it can be derived from the relative proportions of different bigrams (ordered pairs of tokens) occurring in a large text corpus.\n",
        "\n",
        "Let us be a bit more precise in our definition of the bigram model. Let's say that the total size of our vocabulary (the number of tokens we are using) is $V$. Let $A$ be a matrix in $\\mathbb{R}^{V \\times V}$, where each row $A_v$ corresponds to the logits for the prediction of which token would follow a token that has value $v$.\n",
        "Thus, we are modeling the distribution of the token following a token that has value $v$ as\n",
        "\\begin{align*}\n",
        "y_{t+1} \\mid y_t &= v \\sim \\mathrm{Cat}(\\mathbf{\\pi}) \\\\\n",
        "\\pi &=\\mathrm{softmax}(A_v)\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kNWISNQ9bbH"
      },
      "source": [
        "#### Question 1.1.1\n",
        "\n",
        "$\\mathbf{\\pi} \\in \\Delta_{V-1}$ is the vector of probabilities used to parameterize the categorical distribution for the next token prediction. Explain why we parameterize\n",
        "\\begin{equation*}\n",
        "  \\mathbf{\\pi} = \\mathrm{softmax}(A_v),\n",
        "\\end{equation*}\n",
        "and could not just use\n",
        "\\begin{equation*}\n",
        "  \\mathbf{\\pi} = A_v.\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkHP8-T3zJ3D"
      },
      "source": [
        "---\n",
        "\n",
        "**Answer:** Raw logits are unnormalized values, meaning that the sum of each row $A_v$ will likely be unequal to $1$. Because $\\pi$ lives in a probability simplex, we need to apply a normalizer like the softmax function to make sure that the entries of each row $A_v$ are non-negative and sum up to $1$.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtAjylIF-SwQ"
      },
      "source": [
        "#### Question 1.1.2\n",
        "\n",
        "Discuss the relationship between the bigram model and contigency tables (discussed in Lecture 2)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehdR52pnzZHl"
      },
      "source": [
        "---\n",
        "\n",
        "**Answer:** In the context of language modeling, we can interpret contingency tables as a tool for estimating the distribution of different bigrams from a text corpus. The row indices of a contingency model are the first word of a bigram whereas the column indices are the second word of a bigram. The matrix entries are the counts of the corresponding bigram in the training data. Linking back to the above discussion, we can use the contingency obtained this way as the matrix $A$.\n",
        "\n",
        "Contingency tables provide a structured way to represent the data that a bigram model needs. By organizing word occurrences into a table, it becomes straightforward to access and visualize the counts needed for probability calculations, facilitating the model's training and usage.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4AzpB8M-u9o"
      },
      "source": [
        "#### Question 1.1.3\n",
        "\n",
        "Say I have a string of three tokens with ids $(7, 3, 6)$. If I use the bigram model as a generative model for language, given this information, what is distribution of the fourth token?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs4rWYjlzc7n"
      },
      "source": [
        "---\n",
        "\n",
        "**Answer:** Suppose our entire vocabulary is $(7,3,6)$. Then the string of three tokens gives rise to the following contingency table $A$:\n",
        "\n",
        "|     | 7  | 3  | 6  |\n",
        "|-----|----|----|----|\n",
        "| **7** | 0 | 1 | 0  |\n",
        "| **3** | 0 | 0 | 1  |\n",
        "| **6** | 0 | 0 | 0  |\n",
        "\n",
        "If we were to apply the softmax function to each row of the matrix, we see that the distribution of the fourth token is a discrete uniform distribution over the entire vocabulary (i.e., $1/3$ for each token).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leN9gUxoEuhg"
      },
      "source": [
        "#### Question 1.1.4\n",
        "\n",
        "Remember back in Section 0.2 \"Tokenizing the data\" when we gave you the helper function `get_batch`? Run `get_batch` and look at the inputs `x` and the targets `y`. Explain any relation between them in the context of formulating language modeling in the context of next token prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rzLupp2jExDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2a00691-d631-4478-f4f4-ff7194c75561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the features have token ids tensor([[ 8113,  7989,   125,  3687,   325,  1491,  1975,   105, 21047,  6630]],\n",
            "       device='cuda:0')\n",
            "\n",
            "\n",
            "the targets have token ids tensor([[ 7989,   125,  3687,   325,  1491,  1975,   105, 21047,  6630, 19900]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "xb, yb = get_batch('train', 10, device, batch_size = 1)\n",
        "print(f\"the features have token ids {xb}\")\n",
        "print('\\n')\n",
        "print(f\"the targets have token ids {yb}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** `x` represents the first words in bigram pairs, whereas `y` represents the corresponding second words in bigram pairs. The targets vector is a $1$-offset version of the inputs vector, which is what we would expect for a model that performs next token prediction. The formatting of the training data reflects our objective of fitting a model that will predict the most likely next token, given the previous token. The fact that `y` is just an offset version of `x` also illustrates the convenient generation of bigram training data from a text corpus."
      ],
      "metadata": {
        "id": "fcLn5fjFY133"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyYl8QQt_Uhb"
      },
      "source": [
        "#### Question 1.1.5\n",
        "\n",
        "Discuss the strengths and weaknesses of the bigram model as a generative model for language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t6Hc2dhzdzL"
      },
      "source": [
        "---\n",
        "\n",
        "Some strengths of the bigram model:\n",
        "- **Simplicity and Efficiency**: Bigram models are straightforward to implement and understand. They rely on the conditional probability of a word given its immediate predecessor, making them computationally efficient compared to more complex models. This simplicity allows quick training and prediction processes, which is especially useful for applications with limited computational resources.\n",
        "- **Context Awareness**: Although limited, bigram models incorporate a basic level of context by considering the previous word in generating the next word. This is a step up from unigram models, which treat each word occurrence as independent, and can lead to more coherent text generation than purely random word selection.\n",
        "\n",
        "Some weaknesses of the bigram model:\n",
        "- **Limited Context**: A major limitation of bigram models is their consideration of only the immediate previous word for predicting the next word. This severely restricts their ability to capture longer dependencies or the overall context of a sentence. Consequently, bigram models often output grammatically incorrect or nonsensical text.\n",
        "\n",
        "- **Sparse Data Problem**: Bigram models suffer from the issue of data sparsity. Many possible word pairs may not occur in the training corpus, leading to zero probabilities for these unseen bigrams. This issue necessitates the use of smoothing techniques, which can introduce their own complexities and biases.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjYO1XTM_8NE"
      },
      "source": [
        "#### Question 1.1.6\n",
        "\n",
        "Say I have a string $s$ of length $T$. Derive the formula for the negative log likelihood of $s$ under the bigram model in terms of the matrix of logits $A$. What would your answer be if the matrix of logits $A$ were all zeros? What would be the value of the negative log likelihood of $s$ under a model that always perfectly predicted the next token?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuzLfI4Pzes6"
      },
      "source": [
        "---\n",
        "\n",
        "**Answer:** Let $\\mathbb{V}$ be our tokens set with size $V$. Let $s = x_1 x_2 x_3 \\cdots x_T$, where $x_i \\in [V]$. $A \\in \\mathbb{R}^{V \\times V}$ is matrix of logits where each row $A_v$ corresponds to the logits for the prediction of the next token after token $v$. Without loss of generality, we assume that $x_1$ always corresponds to the `<s>` token, while $x_T$ always represents the `<\\s>` token. The likelihood of observing $s$, conditioned on the logits matrix being $A$, is $P(\\{x_t\\}_{t=1}^{T} \\mid A)$. This can be further expanded into\n",
        "\\begin{align*}\n",
        "P(\\{x_t\\}_{t=1}^{T} \\mid A) &= \\prod_{t=1}^{T-1} P(x_{t+1} \\mid x_{t}; A),\n",
        "\\end{align*}\n",
        "where we omit the prior probability $P(x_1)$ since $x_1$ is fixed. The transition probability is:\n",
        "\\begin{align*}\n",
        "P(x_{t+1} \\mid x_{t}; A) &= \\sigma(A_{x_t})_{x_{t+1}}  \\\\\n",
        "    &= \\frac{\\exp(A_{x_t,x_{t+1}})}{\\sum_{v=1}^V \\exp(A_{x_t, v})}.\n",
        "\\end{align*}\n",
        "\n",
        "The negative log likelihood of $s$ under the bigram model is the negative log of the above expression, which is evaluated as\n",
        "\n",
        "\\begin{align*}\n",
        "L   &= - \\sum_{t=1}^{T-1} \\log P(x_{t+1} \\mid x_{t}; A) \\\\\n",
        "    &= - \\sum_{t=1}^{T-1} \\log \\frac{\\exp(A_{x_t,x_{t+1}})}{\\sum_{v=1}^V \\exp(A_{x_t, v})} \\\\\n",
        "    &= - \\sum_{t=1}^{T-1} \\log \\exp(A_{x_t,x_{t+1}}) + \\sum_{t=1}^{T-1} \\log [\\sum_{v=1}^{V} \\exp(A_{x_t, v})] \\\\\n",
        "    &= - \\sum_{t=1}^{T-1} A_{x_t,x_{t+1}} + \\sum_{t=1}^{T-1} \\log [\\sum_{v=1}^{V} \\exp(A_{x_t, v})]\n",
        "\\end{align*}\n",
        "\n",
        "When $A$ is a zero matrix, the above expression of $L$ evaluates to $- (T-1) \\log (\\frac{1}{V})$, or approximately $\\log V$ per token. If a model always perfectly predicted the next token, the negative log likelihood of $s$ will evaluate to $0$.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "x22uMxAssNEf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfzoXAATAfwn"
      },
      "source": [
        "#### Question 1.1.7: Implement the BigramLanguageModel\n",
        "\n",
        "Implement the bigram language model below.\n",
        "\n",
        "Your TODOs:\n",
        "  * if the `forward` method is provided a target, the loss should be the negative log likelihood of the target (given the context)\n",
        "  * `generate` should take in (batched) contexts and a number of new tokens to generate, and then generate text from your model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_lJ1mR4K1aj_"
      },
      "outputs": [],
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          vocab_size: size of the vocabulary (the number of tokens)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.logits_table = nn.Embedding(vocab_size, vocab_size)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def forward(self, token_ids, targets=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: Int(B, T), token ids that make up the context (batch has size B, each entry in the batch has length T)\n",
        "          targets: Int(B, T), token ids corresponding to the target of each context in token_ids\n",
        "\n",
        "        Returns:\n",
        "          logits: (B, T, V), logits[b,t, :] gives the length V vector of logits for the next token prediction in string b up to t tokens\n",
        "          loss: scalar, negative log likelihood of target given context\n",
        "        \"\"\"\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.logits_table(token_ids) # (B,T,V)\n",
        "        log_probits = F.log_softmax(logits, dim = 2) # log of the softmax along the dimension with V elements\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            log_condit_probs = torch.gather(log_probits, dim = 2,\n",
        "                                            index = targets.unsqueeze(2)).squeeze(2)\n",
        "            loss = - log_condit_probs.mean()\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, token_ids, max_new_tokens=context_window_size):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: (B, T) tensor of token ids to provide as context\n",
        "          max_new_tokens: int, maximum number of new tokens to generate\n",
        "\n",
        "        Returns:\n",
        "          (B, T+max_new_tokens) tensor of context with new tokens appended\n",
        "        \"\"\"\n",
        "        # TODO: your code below\n",
        "        new_token_ids = torch.zeros(token_ids.shape[0], max_new_tokens,\n",
        "                                    dtype = torch.int64) # (B, max_new_tokens)\n",
        "        new_token_ids = new_token_ids.to(device)\n",
        "        # Markovian assumption: only the last token matter\n",
        "        logits = self.logits_table(token_ids[:, -1]) # (B, V)\n",
        "        new_token_ids[:,0] = torch.argmax(logits, dim = 1)\n",
        "\n",
        "        if max_new_tokens == 1:\n",
        "            return torch.concat((token_ids, new_token_ids), dim = 1)\n",
        "\n",
        "        for t in range(1, max_new_tokens):\n",
        "            logits = self.logits_table(new_token_ids[:,t-1])\n",
        "            new_token_ids[:,t] = torch.argmax(logits, dim = 1)\n",
        "        return torch.concat((token_ids, new_token_ids), dim = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MaUUxb0EQY1"
      },
      "source": [
        "\n",
        "#### Question 1.1.8: Evaluating the initialization.\n",
        "\n",
        "Evaluate the loss of your untrained bigram model on a batch of data. Does this loss make sense in the context of your answer to Question 1.1.6? Discuss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xgSEGqKGEiOn"
      },
      "outputs": [],
      "source": [
        "x,y = get_batch(\"train\", context_window_size, device)\n",
        "bigram_model = BigramLanguageModel(vocab_size)\n",
        "bm = bigram_model.to(device)\n",
        "\n",
        "# TODO: your code below\n",
        "logits, loss = bm.forward(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(torch.cuda.memory_allocated() / (1024 ** 3), torch.cuda.memory_reserved() / (1024 ** 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVo0l4S3Zgar",
        "outputId": "45658530-0f58-4bee-bf5e-f7c3e3b66f84"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3.1399245262145996, 3.14453125)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Per-token loss of untrained bigram model on a batch of data:\")\n",
        "print(loss.item())\n",
        "print(\"-log(vocab_size):\")\n",
        "print(torch.log(torch.tensor(vocab_size)).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36Vo0fJ0Y82-",
        "outputId": "0d97b6bb-3d20-4f25-88df-f5f3354ccc40"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-token loss of untrained bigram model on a batch of data:\n",
            "10.291149139404297\n",
            "-log(vocab_size):\n",
            "9.997432708740234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS32m6egzg03"
      },
      "source": [
        "---\n",
        "\n",
        "My answer to Question 1.1.6 suggests that the per-token loss of the untrained bigram model will have a loss that is approximately $\\log V$, if the transition matrix were to be initialized as all zeros. In the above calculations, we see that the actual loss ($10.87$) is quite close to $\\log V$ ($9.99$), suggesting that our bigram model is well-specified.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzxRpcgKE4_5"
      },
      "source": [
        "#### Question 1.1.9: Training your bigram model\n",
        "\n",
        "Train your bigram model for `SMALL_ITERS` iterations. Plot and interpret the loss curve.\n",
        "\n",
        "Our train loss gets down to around 4 in around 5 min of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "E2r390zbyz3O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bc9b7b1-b105-4866-8c15-291adf58c640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0\n",
            "step 0: train loss 10.2954, val loss 10.2864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|        | 200/1000 [00:15<00:51, 15.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|        | 202/1000 [00:18<06:05,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: train loss 7.9793, val loss 8.0756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|      | 400/1000 [00:31<00:38, 15.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|      | 402/1000 [00:33<04:34,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: train loss 6.0098, val loss 6.2547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|    | 600/1000 [00:46<00:25, 15.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|    | 602/1000 [00:49<03:02,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: train loss 4.7709, val loss 5.0849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|  | 800/1000 [01:02<00:12, 15.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|  | 802/1000 [01:04<01:30,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: train loss 4.2866, val loss 4.7141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 998/1000 [01:17<00:00, 15.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1000/1000 [01:20<00:00, 12.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 999: train loss 3.9978, val loss 4.5336\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# create a PyTorch optimizer\n",
        "learning_rate = 1e-2\n",
        "optimizer = torch.optim.AdamW(bigram_model.parameters(), lr=learning_rate)\n",
        "\n",
        "eval_interval = 200\n",
        "eval_iters = 200\n",
        "\n",
        "loss_list = []\n",
        "\n",
        "for it in tqdm(range(SMALL_ITERS)):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if it % eval_interval == 0 or it == SMALL_ITERS - 1:\n",
        "        print(f\"iteration {it}\")\n",
        "        losses = estimate_loss(bm, eval_iters, context_window_size, device)\n",
        "        print(f\"step {it}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train', context_window_size, device)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = bm(xb, yb)\n",
        "    loss_list.append(loss.detach().item())\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss_list)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Training loss\")\n",
        "plt.title(\"Per-token training loss\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "H4BQdX5VuRtE",
        "outputId": "88e2726d-1ffa-4ce6-ef74-9b4d8df075bf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Training loss')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj28W2w_ziDA"
      },
      "source": [
        "---\n",
        "\n",
        "**Answer:** Over time, the training loss lowers from 10 and converges to a value around 4.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slaOmWgFLdMk"
      },
      "source": [
        "### 1.2: Token Embeddings: going from discrete tokens to continuous latent spaces\n",
        "\n",
        "In the look up table formulation of the bigram model, we are modelling the logits of the next token didstirbution independently for each token, even if two tokens are extremely similar to each other.\n",
        "One way arond this problem is to learn an embedding of the discrete tokens into $\\mathbb{R}^{D}$, and then to run multi-class logistic regression on top of this learned embedding.\n",
        "\n",
        "More precisely, if we have a vocabulary of tokens of size $V$ that we choose to embed in a Euclidean embedding space of dimension $D$, we can parameterize the distribution of the next token if the current token is $v$ according to\n",
        "\\begin{align*}\n",
        "  \\mathrm{Cat}\\Big( \\mathrm{softmax} (\\beta X_v) \\Big),\n",
        "\\end{align*}\n",
        "where $X_v \\in \\mathbb{R}^{D}$ is the learned embedding of token $v$ into $\\mathbb{R}^{D}$ and $\\beta \\in \\mathbb{R}^{V \\times D}$. Notice that if $X$ were a fixed design matrix this formulation would be equivalent to multi-class logistic regression. However, both $X$ and $\\beta$ are learnable parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX6BRU07O7Cr"
      },
      "source": [
        "#### Question 1.2.1: Implement BigramWithWordEmbeddingsLM\n",
        "\n",
        "Implement a bigram languge model that uses a linear readout from a low dimensional Euclidean embedding of each token to parameterize the logits of the next token distribution, instead of parameterizing the logits of the next token distribution directly. It should have almost the same implementation as `BigramLanguageModel` from Question 1.1.6, except `init` should also take in an `embed_size`, and the `forward` method will need to be modified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "brU5zILoQASX"
      },
      "outputs": [],
      "source": [
        "class BigramWithWordEmbeddingsLM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size=32):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vocab_size: int, size of the vocabulary\n",
        "            embed_size: int, dimension of the word embedding (D)\n",
        "        \"\"\"\n",
        "        # TODO, your code here\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.Euclid_embeddings = nn.Embedding(vocab_size, embed_size) # (V, D)\n",
        "        self.beta = nn.Parameter(torch.randn(vocab_size, embed_size)) # (V, D)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def forward(self, token_ids, targets=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: (B, T) token ids that make up the context (batch has size B, each entry in the batch has length T)\n",
        "          targets: (B, T) token ids corresponding to the target of each context in token_ids\n",
        "\n",
        "        Returns:\n",
        "          logits: (B, T, V), logits[b,t, :] gives the length V vector of logits for the next token prediction in string b up to t tokens\n",
        "          loss: scalar, negative log likelihood of target given context\n",
        "        \"\"\"\n",
        "        # TODO, your code here\n",
        "        X_vs = self.Euclid_embeddings(token_ids) # (B, T, D)\n",
        "        logits = torch.einsum('btd,vd->btv', X_vs, self.beta) # (B, T, V)\n",
        "        log_probits = F.log_softmax(logits, dim = 2) # log of the softmax along the dimension with V elements\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            log_condit_probs = torch.gather(log_probits, dim = 2,\n",
        "                                            index = targets.unsqueeze(2)).squeeze(2)\n",
        "            loss = - log_condit_probs.mean()\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, token_ids, max_new_tokens=context_window_size):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: (B, T) tensor of token ids to provide as context\n",
        "          max_new_tokens: int, maximum number of new tokens to generate\n",
        "\n",
        "        Returns:\n",
        "          (B, T+max_new_tokens) tensor of context with new tokens appended\n",
        "        \"\"\"\n",
        "        #TODO\n",
        "        # your code below\n",
        "        new_token_ids = torch.zeros(token_ids.shape[0], max_new_tokens,\n",
        "                                    dtype = torch.int64) # (B, max_new_tokens)\n",
        "        new_token_ids = new_token_ids.to(device)\n",
        "        # Markovian assumption: only the last token matter\n",
        "        X_vs = self.Euclid_embeddings(token_ids[:, -1]) # (B, V)\n",
        "        logits = torch.einsum('bd,vd->bv', X_vs, self.beta) # (B, V)\n",
        "        new_token_ids[:,0] = torch.argmax(logits, dim = 1)\n",
        "\n",
        "        if max_new_tokens == 1:\n",
        "            return torch.concat((token_ids, new_token_ids), dim = 1)\n",
        "\n",
        "        for t in range(1, max_new_tokens):\n",
        "            X_vs = self.Euclid_embeddings(token_ids[:, t-1]) # (B, V)\n",
        "            logits = torch.einsum('bd,vd->bv', X_vs, self.beta) # (B, V)\n",
        "            new_token_ids[:,t] = torch.argmax(logits, dim = 1)\n",
        "        return torch.concat((token_ids, new_token_ids), dim = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypOE6LnnGdyu"
      },
      "source": [
        "### 1.3: Attention: Relaxing Markovian assumptions to transmit information across the sequence length\n",
        "\n",
        "A major problem with the bigram models of Sections 1.1 and 1.2 was that they were Markovian: the distribution of the next token was determined entirely by the current token! The attention mechanism provides a way to extract information between the previous tokens in the context to provide a better parameterization for the distribution of the next token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-XHeM1VRO6H"
      },
      "source": [
        "#### Question 1.3.1: Averaging over word embeddings\n",
        "\n",
        "One simple way to pool information from previous tokens would simply be to average the embeddings of all the previous tokens!\n",
        "\n",
        "Your TODO: Add comments to the the code snippet below. Write a description here explaining why the code is mathematically equivalent to averaging the embeddings of previous tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-8W0Xx_zsst"
      },
      "source": [
        "---\n",
        "\n",
        "_your answer here_\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCaS07DtGgCr"
      },
      "outputs": [],
      "source": [
        "# average word embedding via matrix multiply and softmax\n",
        "batch_size = 4              # B\n",
        "context_window_size = 8     # T\n",
        "embed_size = 2              # D\n",
        "\n",
        "# make \"synthetic\" word embeddings (for illustration purposes only)\n",
        "X = torch.randn(batch_size, context_window_size, embed_size)\n",
        "\n",
        "# TODO: comment the code below\n",
        "print(X.shape)\n",
        "tril = torch.tril(torch.ones(context_window_size, context_window_size))\n",
        "attn_weights = torch.zeros((context_window_size, context_window_size))\n",
        "attn_weights = attn_weights.masked_fill(tril == 0, float('-inf'))\n",
        "attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "avgDmbds = attn_weights @ X\n",
        "print(X[0])\n",
        "print(\"\")\n",
        "print(avgDmbds[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIDlnRDGXaie"
      },
      "source": [
        "#### 1.3.2: Single-headed scaled $(Q,K,V)$-attention\n",
        "\n",
        "A more sophisticated approach than simply averaging over previous word embeddings is single-headed (Query, Key, Value) scaled attention.\n",
        "That is, we now summarize the information contained in a length $T$ sequence of tokens that have been embeded into $X \\in \\mathbb{R}^{T \\times D}$ according to\n",
        "\\begin{equation}\n",
        "   \\mathrm{SoftmaxAcrossRows} \\Bigg( \\frac{\\mathrm{CausalMask}\\Big(X U_q^\\top U_k X^\\top \\Big)}{\\sqrt{K}} \\Bigg) \\Big( X V^\\top \\Big),\n",
        "\\end{equation}\n",
        "where $U_q, U_k \\in \\mathbb{R}^{K \\times D}$, $V \\in \\mathbb{R}^{D \\times D}$, and $K$ is the \"head size\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E84BzzxPdwUd"
      },
      "source": [
        "##### Question 1.3.2.1\n",
        "\n",
        "In the limiting case where $U_q$ and $U_k$ are all zeros, and $V = I_{D}$, what does $(U_q, U_k, V)$ attention simplify to?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmFw8z5Dz3T6"
      },
      "source": [
        "---\n",
        "\n",
        "_your answer here_\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cU9BXYPekkt"
      },
      "source": [
        "##### Question 1.3.2.2\n",
        "\n",
        "Imagine we had two matrices $U_q$ and $U_k$, both in $\\mathbb{R}^{K \\times D}$, where every entry was an independent standard normal.\n",
        "\n",
        "What would be the distribution of an element of $U_q^\\top U_k$? What about $U_q^\\top U_k / \\sqrt{K}$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDtilESJz4oC"
      },
      "source": [
        "---\n",
        "\n",
        "_your answer here_\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeKfSrREhgdH"
      },
      "source": [
        "##### Question 1.3.2.3: Implement single-headed scaled $(U_q,U_k,V)$-attention.\n",
        "\n",
        "Complete the below code so the `forward` method returns single-headed scaled $(U_q,U_k,V)$-attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_eBPiT-Yy0q"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size, context_window_size, embed_size=384):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          head_size: int, size of the head embedding dimension (K)\n",
        "          context_window_size: int, number of tokens considered in the past for attention (T)\n",
        "          embed_size: int, size of the token embedding dimension (D)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.head_size = head_size\n",
        "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
        "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
        "        self.value = nn.Linear(embed_size, embed_size, bias=False)\n",
        "\n",
        "        # not a param of the model, so registered as a buffer\n",
        "        self.register_buffer('tril', torch.tril(\n",
        "            torch.ones(context_window_size, context_window_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          x: (B,T,D) tensor of token embeddings\n",
        "\n",
        "        Returns:\n",
        "          (B,T,D) tensor of attention-weighted token embeddings\n",
        "        \"\"\"\n",
        "        # TODO: your code here\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RqGrCB21qUV"
      },
      "source": [
        "##### Question 1.3.2.3: Implement a single-headed attention language model\n",
        "\n",
        "Complete the code below. Note that because the transformer has no idea where tokens are occuring in space, we have also added in position embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REr3aWnS1xJL"
      },
      "outputs": [],
      "source": [
        "class SingleHeadedAttentionLM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, head_size, embed_size=384):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "        vocab_size: int, size of the vocabulary (V)\n",
        "        context_window_size: int, number of tokens considered in the past for attention (T)\n",
        "        head_size: int, size of the head embedding dimension (K)\n",
        "        embed_size: int, size of the token embedding dimension (D)\n",
        "      \"\"\"\n",
        "      super().__init__()\n",
        "      self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "      self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
        "      self.context_window_size = context_window_size\n",
        "\n",
        "      # TODO: your code below\n",
        "      self.atten_head = Head(...)\n",
        "      self.lm_head = nn.Linear(...)\n",
        "\n",
        "    def forward(self, token_ids, targets=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: (B, T) token ids that make up the context (batch has size B, each entry\n",
        "                     in the batch has length T)\n",
        "          targets: (B, T) token ids corresponding to the target of each context in token_ids\n",
        "\n",
        "        Returns:\n",
        "          logits: (B, T, V) logits[b,t] gives the length V vector of logits for the next token\n",
        "                   prediction in string b up to t tokens\n",
        "          loss: scalar, negative log likelihood of target given context\n",
        "        \"\"\"\n",
        "        B, T = token_ids.shape # (batch size, length)\n",
        "        tok_emb = self.token_embedding_table(token_ids) # (B,T,K)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,K)\n",
        "        x = tok_emb + pos_emb # (B,T,K)\n",
        "        x = self.atten_head(x) # (B,T,D)\n",
        "        logits = self.lm_head(x) # (B,T,V)\n",
        "\n",
        "        # TODO: your code here\n",
        "        logits = ...\n",
        "        loss = ...\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, token_ids, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: (B, T) tensor of token ids to provide as context\n",
        "          max_new_tokens: int, maximum number of new tokens to generate\n",
        "\n",
        "        Returns:\n",
        "          (B, T+max_new_tokens) tensor of context with new tokens appended\n",
        "        \"\"\"\n",
        "        #TODO\n",
        "        # your code below\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "End-pUCa4RXe"
      },
      "source": [
        "Train your new `SingleHeadedAttentionLM` for `SMALL_ITERS` training iterations and plot the loss curve.\n",
        "Do you seen an improvement compared to your `BigramLanguageModel`? Discuss.\n",
        "\n",
        "Note: you may want to modify the learning rate. Training for `SMALL_ITERS` with a learning rate of `6e-4`, we can get to a loss of around 3.3 in around 4 min of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "070G_l0E0uG9"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u05NFXpo0wWs"
      },
      "source": [
        "---\n",
        "\n",
        "_your answer here_\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wjXYf-S4Lus"
      },
      "source": [
        "#### 1.3.3: Multi-headed attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIObjMO0Ikp5"
      },
      "source": [
        "##### Question 1.3.3.1: Implement multi-headed attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vb8NU_s6Vfg"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, num_heads, head_size, embed_size=384):\n",
        "        super().__init__()\n",
        "        # TODO, your code below\n",
        "        self.heads = nn.ModuleList(...)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO, your code below\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFCBRay3IuR3"
      },
      "source": [
        "##### Question 1.3.3.2: Implement a multi-headed attention LM\n",
        "\n",
        "Fill in the code below to create a language model that outputs its logits for next token prediction using multi-headed attention. Train your model for `SMALL_ITERS` training iterations. Compare the results with the single-headed attention model. Do you see an improvement?\n",
        "\n",
        "We get to a train loss of around 3 in around 5 mins of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvWHwcCzI1yr"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttentionLM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6):\n",
        "      super().__init__()\n",
        "      self.head_size = embed_size // num_heads\n",
        "      self.context_window_size = context_window_size\n",
        "      # TODO: your code below\n",
        "\n",
        "    def forward(self, token_ids, targets=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: (B, T) token ids that make up the context (batch has size B, each entry in the\n",
        "                     batch has length T)\n",
        "          targets: (B, T) token ids corresponding to the target of each context in token_ids\n",
        "\n",
        "        Returns:\n",
        "          logits: (B, T, V), logits[b,t] gives the length V vector of logits for the next token\n",
        "                  prediction in string b up to t tokens\n",
        "          loss: scalar, negative log likelihood of target given context\n",
        "        \"\"\"\n",
        "        # TODO: your code below\n",
        "        logits = ...\n",
        "        loss = ...\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, token_ids, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          token_ids: (B, T) tensor of token ids to provide as context\n",
        "          max_new_tokens: int, maximum number of new tokens to generate\n",
        "\n",
        "        Returns:\n",
        "          (B, T+max_new_tokens) tensor of context with new tokens appended\n",
        "        \"\"\"\n",
        "        # TODO: your code below\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2tKVYDY1XG5"
      },
      "source": [
        "---\n",
        "\n",
        "_your answer here_\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9XsUWpwG7uA"
      },
      "source": [
        "### 1.4: The Transformer Architecture: combining attention with deep learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GbGqwKWJzOK"
      },
      "outputs": [],
      "source": [
        "# run this cell to initialize this deep learning module that you should use in the code your write later\n",
        "# you don't need to edit this layer\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity\n",
        "        Given to you, you don't need to write any code here!\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_size):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_size, 4 * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * embed_size, embed_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKJxVp6aJb6i"
      },
      "source": [
        "#### Question 1.4.1: Implement a transformer block\n",
        "\n",
        "Complete the code below to implement a transformer block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Jw2I6a8IYOi"
      },
      "source": [
        "To make the your implemenation easier to train, we have added two deep learning best practices:\n",
        "\n",
        "1. Residual connections.\n",
        "In the `forward` method of the `TransformerBlock`, we have made the connections of the residual connection, which of the form\n",
        "\\begin{equation}\n",
        "  x = (I + N)(x),\n",
        "\\end{equation}\n",
        "where $I$ stands for the identity transformation and $N$ stands for some non-linearity. The idea is that every layer is some adjustment of the identity function, which allows gradients to flow through a deep network during back propogation, especially at initialization.\n",
        "\n",
        "2. Prenorm via `LayerNorm`\n",
        "Also in the `forward` method of the `TransformerBlock`, the nonlinearity first applied a `LayerNorm` to its arguments. The `LayerNorm` basically standardizes the neurons in that layer so that they have mean 0 and variance 1. Doing so is very helpful for numerical stability, espeically of the gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUDbIv9eISkf"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\" Transformer block: communication across sequence length, followed by communication across embedding space\n",
        "        Uses multi-headed attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, block_size, embed_size=384, num_heads=6):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(embed_size)\n",
        "        self.ln2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        # TODO: your code below\n",
        "        self.feed_forward = FeedForward(...)\n",
        "        self.mh_attention = ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.mh_attention(self.ln1(x)) # communication over sequence length\n",
        "        x = x + self.feed_forward(self.ln2(x)) # communication across embedding space\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqCCiCYcKMD0"
      },
      "source": [
        "#### Question 1.4.2: Implement your baseline transformer model\n",
        "\n",
        "We now stack 6 `TransformerBlocks` (with a final layer norm applied after the blocks but before the logits) to create our basline `TransformerLM`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2veTg9N3ufJ"
      },
      "outputs": [],
      "source": [
        "class TransformerLM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6, n_layers=6):\n",
        "        \"\"\"\n",
        "          Args:\n",
        "              vocab_size: int, number of tokens in the vocabulary (V)\n",
        "              context_window_size: int, size of the context window (T)\n",
        "              embed_size: int, embedding size (D)\n",
        "              num_heads: int, number of heads (H)\n",
        "              n_layers: int, number of layers (M)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "        self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            TransformerBlock(vocab_size,\n",
        "                             context_window_size,\n",
        "                             embed_size=embed_size,\n",
        "                             num_heads=num_heads)\n",
        "            for _ in range(n_layers)])\n",
        "\n",
        "        # final layer norm\n",
        "        self.ln_f = nn.LayerNorm(embed_size)\n",
        "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "        # good initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, token_ids, targets=None):\n",
        "        \"\"\"\n",
        "        Agrgs:\n",
        "            token_ids: tensor of integers, provides the contet, shape (B, T)\n",
        "            targets: tensor of integers, provides the tokens we are preidcitng, shape (B, T)\n",
        "        \"\"\"\n",
        "        B, T = token_ids.shape\n",
        "\n",
        "        # token_ids and targets are both (B, T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(token_ids) # (B, T, D)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, D)\n",
        "        x = tok_emb + pos_emb # (B, T, D)\n",
        "\n",
        "        # TODO: your code below\n",
        "        logits = ...\n",
        "        loss = ...\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, token_ids, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            token_ids: tensor of integers forming the context, shape (B, T)\n",
        "            max_new_tokens: int, max number of tokens to generate\n",
        "        \"\"\"\n",
        "        # TOOD, your code below\n",
        "        return token_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP8430nWKbZ6"
      },
      "source": [
        "Train your `TransformerLM` for `LARGE_ITERS` iterations and plot the loss curve. You may want to change the learning rate.\n",
        "\n",
        "We used a learning rate of `1e-4` and got to a final train loss of around 2.3 in around 25 mins of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsnbDpdhLeKo"
      },
      "outputs": [],
      "source": [
        "trans = TransformerLM(vocab_size, context_window_size)\n",
        "tlm = trans.to(device)\n",
        "learning_rate = 1e-4\n",
        "# TODO, your code below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jgvwzv5Ko7c"
      },
      "source": [
        "Generate an unconditional sample of length `context_window_size` from your trained `TransformerLM`, and also prompt it with the two prompts we gave you. How does the output look? Discuss?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89-1t6MRLVi4"
      },
      "outputs": [],
      "source": [
        "# the contexts for the different prompts\n",
        "start_context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(f\"shape is {start_context.shape}\")\n",
        "context1 = torch.tensor(from_code_bert(tokenizer.encode(prompt_1_text)[:-1]), device=device).reshape(1, -1) # (1, T)\n",
        "print(f\"shape is {context1.shape}\")\n",
        "context2 = torch.tensor(from_code_bert(tokenizer.encode(prompt_2_text)[:-1])).to(device).reshape(1, -1)\n",
        "print(f\"shape is {context2.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_rxT1iSKzdO"
      },
      "outputs": [],
      "source": [
        "# unconditional generate from the transformer model\n",
        "uncond_gen = (tlm.generate(start_context, max_new_tokens=context_window_size)[0].tolist())\n",
        "print(tokenizer.decode(to_code_bert(uncond_gen)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-FeFwD8K4jj"
      },
      "outputs": [],
      "source": [
        "# conditional generation of newton's method\n",
        "# TODO, your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tA2TnXBzK6mN"
      },
      "outputs": [],
      "source": [
        "# conditional generation of cosine distance\n",
        "# TODO, your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgOzvWFDx_yH"
      },
      "source": [
        "#### Question 1.4.3\n",
        "\n",
        "The negative log-likelihood we have been using to train our models can be expressed as\n",
        "\\begin{equation*}\n",
        "  L = -\\frac{1}{T} \\sum_{t = 1}^{T} \\log p(s[t] | \\text{context})\n",
        "\\end{equation*}\n",
        "for some document $s$, where $s[t]$ is the $t$th token of the doc. The natural language processing (NLP) community often reports the quantity\n",
        "\\begin{equation*}\n",
        "  \\text{perplexity} = \\exp(L).\n",
        "\\end{equation*}\n",
        "\n",
        "Give an intuitive interpretation of what perpelxity is. Does the reported perplexity of your trained `TransformerLM` model make sense in terms of samples it generates? (be sure to distinguish betwen `train` and `validation` perplexity). (*Hint: your answer to Question 1.1.6 may be helpful*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Izr1wTOjzlo"
      },
      "source": [
        "## Part 2: Mini-Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lF3jFrQj1f4"
      },
      "source": [
        "Quick recap: So far we have\n",
        "\n",
        "1. Preprocessed the python code dataset by encoding text into integer tokens.\n",
        "2. Implemented single headed attention and then further generalized to multiheaded attention. We further combined multiheaded attention with deep learning to create the transformer architecture.\n",
        "3. Trained our transformer and generate code output.\n",
        "\n",
        "Up to this point, the performance of our simple language model has clearly made a lot of progress. We can see that our model has learned to generate in the style of python code syntax, although there are many quirks that suggest it will not make a very practical code assistant in its current state.\n",
        "\n",
        "### Project Outline\n",
        "\n",
        "Find some area of possible improvement.\n",
        "We interpret \"improvement\" quite loosely, but it is up to you to state precisely in what sense your proposed innovation might constitute an improvement and to show convincing evidence that your innovation does or does not constitue an improvement according to your definition.\n",
        "For your idea, **formulate a hypothesis** for why this change should result in a better model. **Implement your changes** and **report any findings**.\n",
        "\n",
        "_Notes_: As this assignment is being treated as a project, you should expect training to take longer than previous assignments. However, please use your judgement to decide what is reasonable. We will not expect you to run training procedures that take more than 2 hours on the free Google Colab computing resources and we certainly do not expect you to acquire additional compute. The proposed improvements should not solely rely on increased computing demands, but must be based on the goal of improving the model by more efficiently learning from our data.\n",
        "\n",
        "_Hints_: There are many aspects to assessing our model. For example, not only is quality of generated text important, it is also of interest to reduce costs associated with training.\n",
        "\n",
        "### Deliverables\n",
        "\n",
        "In addition to a pdf of your python notebook, the submission for this project will be a written report no more than 4 pages in length using the [NeurIPS LaTex template](https://neurips.cc/Conferences/2023/PaperInformation/StyleFiles). Your report should include detailed analysis of the hypotheses you chose to test along with any conclusions.\n",
        "\n",
        "The page limit for the report does not include bibliography or appendices. Make sure to keep the \"ready for submission\" option to help us grade anonymously. One of your apprendices should contain a link to any code used to generate the project so that we can grade it (google drive with colab nbs or github repo are both fine). You should have at least one plot in your main text (which is capped at 4 pages).\n",
        "\n",
        "### Data augmentation\n",
        "\n",
        "We got the data for this project from [The Stack](https://huggingface.co/datasets/bigcode/the-stack-dedup). If you'd like, you can definitely train on larger datasets by accessing their dataset of python code (we just scratched the surface). You have to make an account on Hugginface to get a Hugginface access token, but the process is pretty quick."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMq2IE4m5q85"
      },
      "source": [
        "## Submission Instructions\n",
        "\n",
        "You will generate two PDFs: one from parts 0 and 1, which involves completing this colab to create a transformer baseline; and one from the mini-project in part 2, which will be your write-up of no longer than 4 pages.\n",
        "\n",
        "**Combine the two PDFs into a single PDF and submit on gradescope. Tag your PDF correclty.**\n",
        "\n",
        "If you work in a group of two, submit one assignment on gradescope. If you complete the assignment individually, submit as usual."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0078e04487ec4095a3d5bbb7f24db3a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_628f511e1d904726a3e9638fef23d665",
              "IPY_MODEL_4a0442732c4f4d9fb43a0903b5a8370d",
              "IPY_MODEL_1a5d9bd787a94b548a2be19a9dfed185"
            ],
            "layout": "IPY_MODEL_64301a962c25422ba3c1fe2659ae5415"
          }
        },
        "628f511e1d904726a3e9638fef23d665": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efa2cef7e558446aa0503d20b2bf8fc4",
            "placeholder": "",
            "style": "IPY_MODEL_9151ddbea4ee48499b79c0ab1a7c17d2",
            "value": "tokenizer_config.json:100%"
          }
        },
        "4a0442732c4f4d9fb43a0903b5a8370d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_777457e679114d1587314d165e68302f",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2cabd697aa145c693be3c194c3bda9e",
            "value": 25
          }
        },
        "1a5d9bd787a94b548a2be19a9dfed185": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be6dfc1abe5b4b9e801e458840524924",
            "placeholder": "",
            "style": "IPY_MODEL_ce86e53f9a664bcfb6a4449de6c8c9b7",
            "value": "25.0/25.0[00:00&lt;00:00,2.34kB/s]"
          }
        },
        "64301a962c25422ba3c1fe2659ae5415": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efa2cef7e558446aa0503d20b2bf8fc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9151ddbea4ee48499b79c0ab1a7c17d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "777457e679114d1587314d165e68302f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2cabd697aa145c693be3c194c3bda9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be6dfc1abe5b4b9e801e458840524924": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce86e53f9a664bcfb6a4449de6c8c9b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f66a194580fa474a8d95354a402dbed3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b59a009124604a54bf87dcaeb39fbec1",
              "IPY_MODEL_e0d12605e10a40598a65adc10df89db9",
              "IPY_MODEL_09a9125590604af3880fb3f543c22bdb"
            ],
            "layout": "IPY_MODEL_e39f82097a914a7c9318a44db253252a"
          }
        },
        "b59a009124604a54bf87dcaeb39fbec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_def03c6018f14663b69fcba4b453474b",
            "placeholder": "",
            "style": "IPY_MODEL_978832e8e5594382a205286f2f6717dd",
            "value": "config.json:100%"
          }
        },
        "e0d12605e10a40598a65adc10df89db9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_493ac37e4d604c19ac33e8fdb08fbc94",
            "max": 498,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc4605cbeeda4ae59fff1b355f10d392",
            "value": 498
          }
        },
        "09a9125590604af3880fb3f543c22bdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2aebfe85937d447b81827c79475c98ce",
            "placeholder": "",
            "style": "IPY_MODEL_814c99ede2054ad8bafb2d5c97480d48",
            "value": "498/498[00:00&lt;00:00,42.3kB/s]"
          }
        },
        "e39f82097a914a7c9318a44db253252a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "def03c6018f14663b69fcba4b453474b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "978832e8e5594382a205286f2f6717dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "493ac37e4d604c19ac33e8fdb08fbc94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc4605cbeeda4ae59fff1b355f10d392": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2aebfe85937d447b81827c79475c98ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "814c99ede2054ad8bafb2d5c97480d48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eecc20acd520448a9a81f1a63dcfa774": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77c5aa3b095f48d3ba7e733046c39ec0",
              "IPY_MODEL_c3d5ac522ec3408a914d79beb748fe01",
              "IPY_MODEL_8e204b2c650e497d97f573ce0764ed2d"
            ],
            "layout": "IPY_MODEL_da1f8541033446bb88763d6195ac2c1d"
          }
        },
        "77c5aa3b095f48d3ba7e733046c39ec0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26f6acc6fd1f4d8d9c297b35b6747f8d",
            "placeholder": "",
            "style": "IPY_MODEL_8fb7851650964f0fa371c96f42fc8e7b",
            "value": "vocab.json:100%"
          }
        },
        "c3d5ac522ec3408a914d79beb748fe01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4e0f1948aa14a4c826e4ac8d2a97f52",
            "max": 898822,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23e6a31b25cf4193b8267d971e0e9c4b",
            "value": 898822
          }
        },
        "8e204b2c650e497d97f573ce0764ed2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e35d3fbb880488a95e08bb76629e4c8",
            "placeholder": "",
            "style": "IPY_MODEL_7990f0259e234e62be5dc4b25f823870",
            "value": "899k/899k[00:00&lt;00:00,1.81MB/s]"
          }
        },
        "da1f8541033446bb88763d6195ac2c1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26f6acc6fd1f4d8d9c297b35b6747f8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fb7851650964f0fa371c96f42fc8e7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4e0f1948aa14a4c826e4ac8d2a97f52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23e6a31b25cf4193b8267d971e0e9c4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e35d3fbb880488a95e08bb76629e4c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7990f0259e234e62be5dc4b25f823870": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdf2640810bf4ad2be623e635cee8f36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb57cd7623814ebe8eaae987903cedfc",
              "IPY_MODEL_85017df8d6bb4ebebd8f3ee4d3d94796",
              "IPY_MODEL_5656e341dd3c4059b1ceeea1766f0c02"
            ],
            "layout": "IPY_MODEL_5bdebb83b6c04b8ca50aa3a53dde58ac"
          }
        },
        "cb57cd7623814ebe8eaae987903cedfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4b0ccb525614f0295e145b3a98da65a",
            "placeholder": "",
            "style": "IPY_MODEL_beedbc83ae594b5b9c6d51528c682ac5",
            "value": "merges.txt:100%"
          }
        },
        "85017df8d6bb4ebebd8f3ee4d3d94796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc545a240a0c43e48b9bf8b55e2b2f28",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_67a393166d4b4e1f865e1be69f7c1bb0",
            "value": 456318
          }
        },
        "5656e341dd3c4059b1ceeea1766f0c02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53961f6d0fa947d1b8ccc2571d5025ad",
            "placeholder": "",
            "style": "IPY_MODEL_9a56c0878d25429aba20b5f6dfdd7392",
            "value": "456k/456k[00:00&lt;00:00,31.8MB/s]"
          }
        },
        "5bdebb83b6c04b8ca50aa3a53dde58ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4b0ccb525614f0295e145b3a98da65a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "beedbc83ae594b5b9c6d51528c682ac5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc545a240a0c43e48b9bf8b55e2b2f28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67a393166d4b4e1f865e1be69f7c1bb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "53961f6d0fa947d1b8ccc2571d5025ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a56c0878d25429aba20b5f6dfdd7392": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1dbc14a7ae64dd588252ed595a09c20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ebe0e395a3843c6873c129ba1cee2de",
              "IPY_MODEL_38cd9b74f0fb4dba9608ac5c4c2db42a",
              "IPY_MODEL_08848c184d1544f886aac6aad95b064a"
            ],
            "layout": "IPY_MODEL_836aa82ea12143e6ac118ddb55456ca2"
          }
        },
        "1ebe0e395a3843c6873c129ba1cee2de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8efcb07b65334270a203e1376713af54",
            "placeholder": "",
            "style": "IPY_MODEL_532f0155ecbe46589da10aae684754ae",
            "value": "special_tokens_map.json:100%"
          }
        },
        "38cd9b74f0fb4dba9608ac5c4c2db42a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce111c7acd7b4bd0b360dd76c0c58b61",
            "max": 150,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_75acf48645194d4d9e9cbdb01132ed64",
            "value": 150
          }
        },
        "08848c184d1544f886aac6aad95b064a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25c5de389304432c9d44c266ec686297",
            "placeholder": "",
            "style": "IPY_MODEL_ae16294f1a2f404f8c4fa9626739c3c9",
            "value": "150/150[00:00&lt;00:00,11.7kB/s]"
          }
        },
        "836aa82ea12143e6ac118ddb55456ca2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8efcb07b65334270a203e1376713af54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "532f0155ecbe46589da10aae684754ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce111c7acd7b4bd0b360dd76c0c58b61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75acf48645194d4d9e9cbdb01132ed64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "25c5de389304432c9d44c266ec686297": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae16294f1a2f404f8c4fa9626739c3c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}